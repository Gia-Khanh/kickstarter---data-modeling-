# -*- coding: utf-8 -*-
"""Individual Project 446.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qAWaWFvKpG0R1GC9UzNVwghUUHuyK_n5
"""

## Developing the model ###

# Load Libraries
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import statsmodels.api
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV

# Import data
kickstarter_df = pd.read_excel("Kickstarter.xlsx")
kickstarter_df

kickstarter_df = kickstarter_df.dropna()
print('\nShape duplicate: \n', kickstarter_df[kickstarter_df.duplicated()].shape)
kickstarter_df = kickstarter_df.drop_duplicates()

print('\nShape df af_dup: \n', kickstarter_df.shape)

#Remove canceled and suspended state
kickstarter_df = kickstarter_df[kickstarter_df.state !='canceled']
kickstarter_df = kickstarter_df[kickstarter_df.state !='suspended']

#Exclude variables involving state_changed

#Label Encoding of categorical variables
kickstarter_df['deadline_weekday'].replace(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'],[1,2,3,4,5,6,7],inplace=True)
kickstarter_df['created_at_weekday'].replace(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'],[1,2,3,4,5,6,7],inplace=True)
kickstarter_df['launched_at_weekday'].replace(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'],[1,2,3,4,5,6,7],inplace=True)



kickstarter_df['state'].replace(['successful','failed'],[1,0],inplace=True)

kickstarter_df

df=kickstarter_df.drop(['id','name', 'disable_communication','launch_to_state_change_days','state_changed_at_month','staff_pick','state_changed_at_day','state_changed_at_yr','state_changed_at_hr','pledged','currency','name_len_clean','blurb_len_clean','deadline','state_changed_at','created_at','launched_at','backers_count','static_usd_rate','usd_pledged','spotlight','state_changed_at_weekday','deadline_hr','deadline_day','created_at_hr','created_at_day',"launched_at_hr","launched_at_day"], axis=1)
df

# Using get_dummies
for column in df.select_dtypes(include=['object']).columns:
  dum = pd.get_dummies(df[column], prefix=column, drop_first=True)
  df = pd.concat([df, dum], axis=1)


display(df)

df.info()

# Setup the variables
X = df[["goal", "name_len","blurb_len","deadline_weekday","created_at_weekday","launched_at_weekday","deadline_month","deadline_yr" ,"created_at_month","created_at_yr","launched_at_month","launched_at_yr", "category_Apps","category_Blues","category_Experimental","category_Festivals","category_Flight","category_Gadgets","category_Hardware","category_Immersive","category_Makerspaces","category_Musical","category_Places","category_Plays","category_Robots","category_Shorts","category_Software","category_Sound","category_Spaces","category_Thrillers","category_Wearables","category_Web","category_Webseries","country_AU","country_BE","country_CA","country_CH","country_DE","country_DK","country_ES","country_FR","country_GB","country_IE","country_IT","country_LU","country_NL","country_NO","country_NZ","country_SE","country_US",'create_to_launch_days','launch_to_deadline_days']]
#exclude currency as it may overlap with static_usd_rate
y = df["state"]

# Standardize predictors
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 5)

"""# CLASSIFICATION MODEL

# ANN - Lasso without feature selection
"""

## Cross-validate with different size of the hidden layer
for i in range (1,60):
  mlp = MLPClassifier(hidden_layer_sizes=(i),max_iter=1000, random_state=0, activation = 'logistic')
  model = mlp.fit(X_train,y_train)
  y_test_pred = model.predict(X_test)
  acc=metrics.accuracy_score(y_test, y_test_pred)
  print("Accuracy score using ANN with ",i," neighbors = ", metrics.accuracy_score(y_test, y_test_pred))

#TEST MODEL
mlp = MLPClassifier(hidden_layer_sizes=(31),max_iter=1000, random_state=0, activation = 'logistic')
model = mlp.fit(X_train,y_train)
## Make prediction and evaluate accuracy
y_test_pred = model.predict(X_test)
accuracy_score(y_test, y_test_pred)

"""# ANN with Feature Selection"""

# Run LASSO with alpha=0.01
ls = Lasso(alpha=0.01) # you can control the number of predictors through alpha
model = ls.fit(X_std,y)

fsls = pd.DataFrame(list(zip(X.columns,model.coef_)), columns = ['predictor','coefficient'])
display(fsls)



# Fit the model on new dataset and evaluate
X_lasso_train = X_train[["goal","name_len","blurb_len","deadline_weekday","launched_at_weekday","deadline_yr","created_at_yr","launched_at_month","category_Apps","category_Blues","category_Experimental","category_Festivals","category_Flight","category_Gadgets","category_Immersive","category_Musical","category_Places","category_Plays","category_Robots","category_Shorts","category_Software","category_Sound","category_Spaces","category_Thrillers","category_Web","category_Webseries","country_AU","country_CA", "country_GB","country_US","launch_to_deadline_days"]]
X_lasso_test = X_test[["goal","name_len","blurb_len","deadline_weekday","launched_at_weekday","deadline_yr","created_at_yr","launched_at_month","category_Apps","category_Blues","category_Experimental","category_Festivals","category_Flight","category_Gadgets","category_Immersive","category_Musical","category_Places","category_Plays","category_Robots","category_Shorts","category_Software","category_Sound","category_Spaces","category_Thrillers","category_Web","category_Webseries","country_AU","country_CA", "country_GB","country_US","launch_to_deadline_days"]]

mlp = MLPClassifier(hidden_layer_sizes=(31),max_iter=1000, random_state=0, activation = 'logistic')

## Build ANN model
model = mlp.fit(X_lasso_train,y_train)
y_test_pred = model.predict(X_lasso_test)
accuracy_ann_lasso = accuracy_score(y_test, y_test_pred)
print('accuracy_ann_lasso : ', accuracy_ann_lasso)



"""# Random forest without feature selection"""

# Build the model
randomforest = RandomForestClassifier(random_state=0)
model_rf = randomforest.fit(X_train, y_train)

# Make prediction and evaluate accuracy
y_test_pred = model_rf.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_test_pred)
print('Accuracy rand. forest: ', accuracy_rf)

"""# Random forest with feature selection"""

## Random forest
randomforest = RandomForestClassifier(random_state=0)
model = randomforest.fit(X, y)

# Print feature importance
pd.Series(model.feature_importances_,index = X.columns).sort_values(ascending = False).plot(kind = 'bar', figsize = (14,6))

# Can specify a threshold for the feature importance; common is 0.05 but it's always subjective
firf = pd.DataFrame(list(zip(X.columns,model.feature_importances_)), columns = ['predictor','feature importance'])
display(firf)

# Fit the model on new dataset and evaluate
X_rf_train = X_train[['goal','create_to_launch_days','blurb_len','name_len','launch_to_deadline_days','created_at_month','created_at_weekday','deadline_month','launched_at_month','deadline_weekday','launched_at_weekday','category_Web','deadline_yr','category_Software','created_at_yr','launched_at_yr','category_Plays','country_US','category_Hardware','category_Musical','country_GB','category_Festivals','category_Gadgets','category_Apps','category_Wearables']]
X_rf_test = X_test[['goal','create_to_launch_days','blurb_len','name_len','launch_to_deadline_days','created_at_month','created_at_weekday','deadline_month','launched_at_month','deadline_weekday','launched_at_weekday','category_Web','deadline_yr','category_Software','created_at_yr','launched_at_yr','category_Plays','country_US','category_Hardware','category_Musical','country_GB','category_Festivals','category_Gadgets','category_Apps','category_Wearables']]

model = mlp.fit(X_rf_train,y_train)
y_test_pred = model.predict(X_rf_test)
accuracy_ann_rf = accuracy_score(y_test, y_test_pred)
print('accuracy_ann_rf : ', accuracy_ann_rf)

"""# Random Forest with GridSearchCv"""

param_grid = {'n_estimators': [20, 30, 40, 50], 'max_features': [7,8,9,10],'min_samples_leaf':[5,6,7,8]}
rf_Model=RandomForestClassifier(random_state=0)
rf_Grid=GridSearchCV(estimator=rf_Model, param_grid=param_grid, cv=5, verbose=True)

fit_grid = rf_Grid.fit(X, y)
best_classifier = fit_grid.best_estimator_
best_classifier

best_parameters = fit_grid.best_params_
best_parameters

# Make prediction and evaluate accuracy

randomforest = RandomForestClassifier(n_estimators=50, max_features=9, min_samples_leaf=4, random_state=0, oob_score=True)
model_rf=randomforest.fit(X, y)
print('Accuracy score: ', model_rf.oob_score_)

"""# Logistic regression"""

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 5)

#Build the model
lr = LogisticRegression()
model_lr = lr.fit(X_train,y_train)

# View results
model_lr.intercept_
model_lr.coef_

# Using the model to predict the results based on the test dataset
# predict() will return the labels resulting for each data point
y_test_pred = model_lr.predict(X_test)

# Using the model to predict the probability of being classified to each category

y_test_pred_prob = model_lr.predict_proba(X_test)

# Get performance measures
# Accuracy score
print('Accuracy score: \t', metrics.accuracy_score(y_test, y_test_pred))

"""# K-NN MODEL"""

## Choosing k
for i in range (50,150):
    knn = KNeighborsClassifier(n_neighbors=i)
    model = knn.fit(X_train,y_train)
    y_test_pred = model.predict(X_test)
    print("Accuracy score using k-NN with ",i," neighbors = ", metrics.accuracy_score(y_test, y_test_pred))



#TEST MODEL
knn = KNeighborsClassifier(n_neighbors=84)
model_knn = knn.fit(X_train,y_train)

# Using the model to predict the results based on the test dataset
y_test_pred = model_knn.predict(X_test)

print("Accuracy score using k-NN with neighbors = 84 is ", metrics.accuracy_score(y_test, y_test_pred))

"""# Classification tree

---


"""

# Pruning pre-model building using K-fold cross validation for trees with different depths (Pre-Pruning)
for i in range (2,20):
    model = DecisionTreeClassifier(max_depth=i)
    scores = cross_val_score(estimator=model, X=X, y=y, cv=5)
    print(i,':',np.average(scores))

# Pruning post-model building (Post-Pruning)
ct = DecisionTreeClassifier()
path = ct.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
ccp_alphas = ccp_alphas[ccp_alphas >= 0]

accuracy_ct = []
for ccp_alpha in ccp_alphas:
    ct = DecisionTreeClassifier(ccp_alpha=ccp_alpha)
    model = ct.fit(X_train, y_train)
    y_test_pred = model.predict(X_test)
    accuracy_ct.append(accuracy_score(y_test, y_test_pred))

accuracy_ct = np.array(accuracy_ct)
alpha_accuracy = pd.DataFrame({'ccp_alpha': ccp_alphas, 'accuracy': accuracy_ct}, columns=['ccp_alpha', 'accuracy'])
accuracy_ccp = alpha_accuracy['accuracy'].max()
best_ccp_alphas = alpha_accuracy[alpha_accuracy['accuracy'] == accuracy_ccp]
ccp_alpha = alpha_accuracy[alpha_accuracy['accuracy'] == accuracy_ccp]['ccp_alpha'].values[0]

display(alpha_accuracy)

print('Max accuracy: ', accuracy_ccp)

"""# Gradient boosting

Model without feature selection
"""

# Build the model
gbt = GradientBoostingClassifier(random_state=0)
model_gbt = gbt.fit(X_train, y_train)

pd.Series(model_gbt.feature_importances_,index = X.columns).sort_values(ascending = False).plot(kind = 'bar', figsize = (14,6))

# Make prediction and evaluate accuracy

y_test_pred = model_gbt.predict(X_test)
accuracy_gbt = accuracy_score(y_test, y_test_pred)
print('Accuracy score: ', accuracy_gbt)

"""Model with feature selection"""

# Build the model
gbt = GradientBoostingClassifier(random_state=0)
X_gbt_train = X_train[['goal','category_Web','category_Software','create_to_launch_days','name_len','launch_to_deadline_days','launched_at_yr','category_Places','category_Plays']]
X_gbt_test = X_test[['goal','category_Web','category_Software','create_to_launch_days','name_len','launch_to_deadline_days','launched_at_yr','category_Places','category_Plays']]
model_gbt = gbt.fit(X_gbt_train, y_train)


y_test_pred = model_gbt.predict(X_gbt_test)
accuracy_gbt = accuracy_score(y_test, y_test_pred)
print('Accuracy score: ', accuracy_gbt)



"""# CLUSTERING MODEL

K-Means
"""

kickstarter_df

A=kickstarter_df.iloc[:,[2,3,4,5,6,7,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44]]

# Using get_dummies
for column in A.select_dtypes(include=['object']).columns:
  dum = pd.get_dummies(A[column], prefix=column, drop_first=True,dtype=int)
  A = pd.concat([A, dum], axis=1)


display(A)

B=A.drop(['disable_communication','country','currency','staff_pick','category','spotlight','deadline_weekday','state_changed_at_weekday','created_at_weekday','launched_at_weekday'],axis=1)
B

# Setup the variables using every available columns except id
X_cl = B.iloc[:,0:200]
y = df["state"]

# Standardize predictors
scaler = StandardScaler()
X_cl_std = scaler.fit_transform(X_cl)

## Principal component analysis
pca = PCA(n_components=2)
pca.fit(X_cl_std)

# Check the values of the eigen values
pca.explained_variance_ratio_
pca.components_

plt.scatter(pca.components_[0],pca.components_[1])

# Model building
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

"""# Agglomerative Clustering"""

cluster = AgglomerativeClustering(n_clusters=3, linkage='complete')
cluster.fit_predict(X_cl_std)
print(cluster.labels_)

pd.DataFrame(list(zip(kickstarter_df['name'],np.transpose(cluster.labels_))), columns = ['Project Name','Cluster label'])

"""# KMeans Clustering"""

kmeans = KMeans(n_clusters=3)
model = kmeans.fit(X_cl_std)
labels = model.predict(X_cl_std)

x=pd.DataFrame(list(zip(kickstarter_df['name'],np.transpose(labels))), columns = ['Project name','Cluster label'])
x

x['Cluster label'].value_counts()

df['country'].value_counts()

centroids = kmeans.cluster_centers_
print(centroids)

# Model building

from sklearn.metrics import silhouette_samples
from sklearn.metrics import silhouette_score


# Silhouette analysis when k=3
kmeans = KMeans(n_clusters=3)
model = kmeans.fit(X_cl_std)
labels = model.labels_


silhouette = silhouette_samples(X_cl_std,labels)

df = pd.DataFrame({'label':labels,'silhouette':silhouette})

print('Average Silhouette Score for Cluster 0: ', np.average(df[df['label'] == 0].silhouette))
print('Average Silhouette Score for Cluster 1: ', np.average(df[df['label'] == 1].silhouette))
print('Average Silhouette Score for Cluster 1: ', np.average(df[df['label'] == 2].silhouette))


silhouette_score(X_cl_std,labels)

